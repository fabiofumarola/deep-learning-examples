{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram word2vec\n",
    "\n",
    "In this notebook, I'll lead you through using TensorFlow to implement the word2vec algorithm using the skip-gram architecture. By implementing this, you'll learn about embedding words for use in natural language processing. This will come in handy when dealing with things like translations.\n",
    "\n",
    "## Readings\n",
    "\n",
    "Here are the resources I used to build this notebook. I suggest reading these either beforehand or while you're working on this material.\n",
    "\n",
    "* A really good [conceptual overview](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) of word2vec from Chris McCormick \n",
    "* [First word2vec paper](https://arxiv.org/pdf/1301.3781.pdf) from Mikolov et al.\n",
    "* [NIPS paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) with improvements for word2vec also from Mikolov et al.\n",
    "* An [implementation of word2vec](http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/) from Thushan Ganegedara\n",
    "* TensorFlow [word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)\n",
    "\n",
    "## Word embeddings\n",
    "\n",
    "When you're dealing with language and words, you end up with tens of thousands of classes to predict, one for each word. Trying to one-hot encode these words is massively inefficient, you'll have one element set to 1 and the other 50,000 set to 0. The word2vec algorithm finds much more efficient representations by finding vectors that represent the words. These vectors also contain semantic information about the words. Words that show up in similar contexts, such as \"black\", \"white\", and \"red\" will have vectors near each other. There are two architectures for implementing word2vec, CBOW (Continuous Bag-Of-Words) and Skip-gram.\n",
    "\n",
    "<img src=\"assets/word2vec_architectures.png\" width=\"500\">\n",
    "\n",
    "In this implementation, we'll be using the skip-gram architecture because it performs better than CBOW. Here, we pass in a word and try to predict the words surrounding it in the text. In this way, we can train the network to learn representations for words that show up in similar contexts.\n",
    "\n",
    "First up, importing packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "# import text_utils methods\n",
    "import text_utils\n",
    "import utils\n",
    "\n",
    "from collections import Counter, namedtuple\n",
    "import random\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [text8 dataset](http://mattmahoney.net/dc/textdata.html), a file of cleaned up Wikipedia articles from Matt Mahoney. The next cell will download the data set to the `data` folder. Then you can extract it and delete the archive file to save storage space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the vocabolary\n",
    "\n",
    "we restore the vocab_to_int and the int_to_vocab using the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('dictionary.cpkt', 'rb') as f:\n",
    "    dictionaries = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int = dictionaries['vocab_to_int']\n",
    "int_to_vocab = dictionaries['int_to_vocab']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model\n",
    "\n",
    "we build the model as before, but we are going to restore a saved checkpoint `word2vec_300.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = './checkpoints/word2vec_300.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embed_dim, num_sampled=100):\n",
    "    with tf.name_scope('Inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "\n",
    "    with tf.name_scope('Labels'):\n",
    "        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "\n",
    "    with tf.name_scope('Embedding'):\n",
    "        embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1.0, 1.0), name='embeddings')\n",
    "        embed = tf.nn.embedding_lookup(embeddings, inputs, name='embed')\n",
    "\n",
    "    with tf.name_scope('NegativeSampling'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.1), name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(vocab_size), name='softmax_b')\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b',softmax_b)\n",
    "    #negative labels to sample\n",
    "    with tf.name_scope('Cost'):\n",
    "        loss = tf.nn.sampled_softmax_loss(softmax_w,softmax_b,labels,embed, num_sampled,vocab_size, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        tf.summary.scalar('cost',cost)\n",
    "\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(name='optimizer').minimize(cost)\n",
    "        \n",
    "    # merge all the summary in one node\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes\n",
    "    export_nodes = ['inputs', 'labels', 'embeddings', 'embed', \n",
    "                    'softmax_w', 'softmax_b', 'cost', 'optimizer', 'merged']\n",
    "\n",
    "    \n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab_to_int),\n",
    "    embed_dim = 300,\n",
    "    num_sampled = 100\n",
    ")\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the top k nearest neighbour function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_top_k(embeddings, input_codes, top_k = 8):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sample_examples = np.array(input_codes)\n",
    "        sample_dataset = tf.constant(sample_examples, dtype=tf.int32)\n",
    "    \n",
    "        # We use the cosine distance:\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embedding = embeddings / norm\n",
    "        sample_embedding = tf.nn.embedding_lookup(normalized_embedding, sample_dataset)\n",
    "        similarity = tf.matmul(sample_embedding, tf.transpose(normalized_embedding))\n",
    "        sim = similarity.eval()\n",
    "        for c in input_codes:\n",
    "            word = int_to_vocab[c]\n",
    "            nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "            log = 'Nearest to %s:' % word\n",
    "            for k in range(top_k):\n",
    "                close_words = int_to_vocab[nearest[k]]\n",
    "                log = '%s %s,' % (log, close_words)\n",
    "            print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_top_k(model.embeddings, [3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
