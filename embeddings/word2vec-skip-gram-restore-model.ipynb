{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram word2vec\n",
    "\n",
    "In this notebook, I'll lead you through using TensorFlow to implement the word2vec algorithm using the skip-gram architecture. By implementing this, you'll learn about embedding words for use in natural language processing. This will come in handy when dealing with things like translations.\n",
    "\n",
    "## Readings\n",
    "\n",
    "Here are the resources I used to build this notebook. I suggest reading these either beforehand or while you're working on this material.\n",
    "\n",
    "* A really good [conceptual overview](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) of word2vec from Chris McCormick \n",
    "* [First word2vec paper](https://arxiv.org/pdf/1301.3781.pdf) from Mikolov et al.\n",
    "* [NIPS paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) with improvements for word2vec also from Mikolov et al.\n",
    "* An [implementation of word2vec](http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/) from Thushan Ganegedara\n",
    "* TensorFlow [word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)\n",
    "\n",
    "## Word embeddings\n",
    "\n",
    "When you're dealing with language and words, you end up with tens of thousands of classes to predict, one for each word. Trying to one-hot encode these words is massively inefficient, you'll have one element set to 1 and the other 50,000 set to 0. The word2vec algorithm finds much more efficient representations by finding vectors that represent the words. These vectors also contain semantic information about the words. Words that show up in similar contexts, such as \"black\", \"white\", and \"red\" will have vectors near each other. There are two architectures for implementing word2vec, CBOW (Continuous Bag-Of-Words) and Skip-gram.\n",
    "\n",
    "<img src=\"assets/word2vec_architectures.png\" width=\"500\">\n",
    "\n",
    "In this implementation, we'll be using the skip-gram architecture because it performs better than CBOW. Here, we pass in a word and try to predict the words surrounding it in the text. In this way, we can train the network to learn representations for words that show up in similar contexts.\n",
    "\n",
    "First up, importing packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "# import text_utils methods\n",
    "import text_utils\n",
    "import utils\n",
    "\n",
    "from collections import Counter, namedtuple\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [text8 dataset](http://mattmahoney.net/dc/textdata.html), a file of cleaned up Wikipedia articles from Matt Mahoney. The next cell will download the data set to the `data` folder. Then you can extract it and delete the archive file to save storage space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the vocabolary\n",
    "\n",
    "we restore the vocab_to_int and the int_to_vocab using the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('dictionary.cpkt', 'rb') as f:\n",
    "    dictionaries = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int = dictionaries['vocab_to_int']\n",
    "int_to_vocab = dictionaries['int_to_vocab']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model\n",
    "\n",
    "we build the model as before, but we are going to restore a saved checkpoint `word2vec_300.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(graph, vocab_size, embed_dim, num_sampled=100):\n",
    "    with graph.as_default():\n",
    "        with tf.name_scope('Inputs'):\n",
    "            inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "\n",
    "        with tf.name_scope('Labels'):\n",
    "            labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.name_scope('Embedding'):\n",
    "                embeddings = tf.Variable(\n",
    "                    initial_value = tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0), \n",
    "                    name='embeddings')\n",
    "                embed = tf.nn.embedding_lookup(embeddings, inputs, name='embed')\n",
    "\n",
    "            with tf.name_scope('NegativeSampling'):\n",
    "                softmax_w = tf.Variable(\n",
    "                    tf.truncated_normal([vocab_size, embed_dim], stddev=1.0 / math.sqrt(embed_dim)), name='softmax_w')\n",
    "                softmax_b = tf.Variable(tf.zeros([vocab_size]), name='softmax_b')\n",
    "\n",
    "                #negative labels to sample\n",
    "            with tf.name_scope('Loss'):\n",
    "                loss = tf.reduce_mean(\n",
    "                   tf.nn.sampled_softmax_loss(weights=softmax_w,\n",
    "                      biases=softmax_b,\n",
    "                      labels=labels, \n",
    "                      inputs=embed, \n",
    "                      num_sampled=num_sampled,\n",
    "                      num_classes=vocab_size),\n",
    "                    name = 'loss'\n",
    "                    )\n",
    "                tf.summary.scalar('loss',loss)\n",
    "            \n",
    "            optimizer = tf.train.AdamOptimizer(name='optimizer').minimize(loss)\n",
    "        \n",
    "        # merge all the summary in one node\n",
    "        merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes\n",
    "    export_nodes = ['inputs', 'labels', 'embeddings', 'embed', \n",
    "                    'softmax_w', 'softmax_b', 'loss', 'optimizer', 'merged']\n",
    "\n",
    "    \n",
    "    Model = namedtuple('Model', export_nodes)\n",
    "    local_dict = locals()\n",
    "    model = Model(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return model   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Functions\n",
    "\n",
    "This code is from Thushan Ganegedara's implementation. Here we're going to choose a few common words and few uncommon words. Then, we'll print out the closest words to them. It's a nice way to check that our embedding table is grouping together words with similar semantic meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(embeddings, int_codes):\n",
    "    sample_examples = np.array(int_codes)\n",
    "    sample_dataset = tf.constant(sample_examples, dtype=tf.int32)\n",
    "    \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embedding = embeddings / norm\n",
    "    sample_embedding = tf.nn.embedding_lookup(normalized_embedding, sample_dataset)\n",
    "    return tf.matmul(sample_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "\n",
    "model = build_model(\n",
    "    graph = train_graph,\n",
    "    vocab_size = len(vocab_to_int),\n",
    "    embed_dim = 50,\n",
    "    num_sampled = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = './checkpoints/word2vec_1.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 186]\n",
      "Nearest to the: of, in, and, from, to, were, by, at,\n",
      "Nearest to of: the, in, on, from, and, to, most, a,\n",
      "Nearest to and: by, the, as, to, in, they, for, also,\n",
      "Nearest to one: five, nine, two, zero, eight, seven, four, six,\n",
      "Nearest to in: the, of, and, was, were, to, by, after,\n",
      "Nearest to a: is, for, as, which, each, the, interest, on,\n",
      "Nearest to to: the, they, and, that, have, not, as, will,\n",
      "Nearest to zero: two, one, four, six, three, nine, five, eight,\n",
      "Nearest to nine: one, seven, four, six, five, eight, american, zero,\n",
      "Nearest to two: zero, five, one, four, six, three, seven, nine,\n",
      "Nearest to king: had, asserted, strange, turning, rumored, september, descendant, vigor,\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess,model_path)\n",
    "    \n",
    "    sample_examples = list(range(10))\n",
    "    sample_examples.append(186)\n",
    "    sample_size = len(sample_examples)\n",
    "    \n",
    "    sim = validate(model.embeddings,sample_examples).eval()\n",
    "    for i in range(sample_size):\n",
    "        sample_word = int_to_vocab[sample_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = 'Nearest to %s:' % sample_word\n",
    "        for k in range(top_k):\n",
    "            close_word = int_to_vocab[nearest[k]]\n",
    "            log = '%s %s,' % (log, close_word)\n",
    "        print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
